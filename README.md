#  ğŸ§  ScienceSage
Smart Science, Made Simple

An end-to-end **Retrieval-Augmented Generation (RAG)** project built for the [LLM Zoomcamp Capstone](https://github.com/DataTalksClub/llm-zoomcamp).  
This system helps users **explore complex scientific topics** (like neuroplasticity, AI concepts, renewable energy, animal behavior, and ecosystem interactions) at **different levels of explanation**:

- ğŸ« **Middle School** (simple, intuitive)  
- ğŸ“ **College** (intermediate, with more depth)  
- ğŸ§ª **Advanced** (detailed, technical)

Powered by **GPT-4**, **Qdrant**, and **Streamlit**, and developed in **Codespaces (Python 3.12)**.

---

## âœ¨ Features
- **End-to-end RAG pipeline** (OpenAI GPT + Qdrant vector DB).  
- **Multi-level answers** (simple â†’ advanced).  
- **Public domain data sources** (NASA, Wikipedia, Stanford).  
- **Feedback system** (ğŸ‘ / ğŸ‘ per answer, stored for analysis).  
- **Streamlit interface** with example queries and sidebar controls.  

---

## ğŸ”¹ Topics
- **Neuroplasticity** (psych/neuro)
- **AI Concepts** (transformers, RAG, embeddings, etc.)
- **Renewable Energy & Climate Change**
- **Animal Adaptation/Behavior**
- **Ecosystem Interactions**

---

## ğŸ“‚ Project Structure
```
ScienceSage/
â”‚
â”œâ”€â”€ app/ # Application (Streamlit + backend logic)
â”‚ â”œâ”€â”€ app.py # Streamlit UI
â”‚ â”œâ”€â”€ retrieval_system.py # Query â†’ retrieve â†’ GPT pipeline
â”‚ â”œâ”€â”€ feedback_manager.py # Save thumbs up/down
â”‚ â”œâ”€â”€ analyze_feedback.py # Summarize user feedback
â”‚ â”œâ”€â”€ config.py # API keys & settings
â”‚ â””â”€â”€ prompts.py # Prompts
â”‚
â”œâ”€â”€ data/ # Data sources & outputs
â”‚ â”œâ”€â”€ raw/ # Original files (html, pdf, etc.)
â”‚ â”œâ”€â”€ processed/ # Clean text files
â”‚ â”œâ”€â”€ chunks/ # JSONL with chunked docs
â”‚ â””â”€â”€ feedback/ # Feedback file for analysis
â”‚
â”œâ”€â”€ notebooks/ # Jupyter exploration
â”‚ â””â”€â”€ sanity_check.ipynb
â”‚
â”œâ”€â”€ scripts/ # Utilities
â”‚ â”œâ”€â”€ download_and_clean.py # Download NASA/Wikipedia/PDF â†’ text
â”‚ â”œâ”€â”€ preprocess.py # Chunk text â†’ JSONL
â”‚ â”œâ”€â”€ embed.py # Embed chunks â†’ Qdrant
â”‚ â””â”€â”€ test_qdrant.py # Sanity check retrieval
â”‚
â”œâ”€â”€ docker/ # Docker setup
â”‚ â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ tests/ # Unit/integration tests
â”‚ â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ .env.example # Example API keys (not committed)
â””â”€â”€ .gitignore
```

---

## ğŸ“Š Data Sources (Public Domain)

- **Neuroplasticity**: [Wikipedia](https://en.wikipedia.org/wiki/Neuroplasticity)  
- **AI Concepts**:  
  - [Wikipedia â€“ Transformer (ML)](https://en.wikipedia.org/wiki/Transformer_(machine_learning))  
  - [Stanford CS324 LLM Lectures (CC BY-SA)](https://web.stanford.edu/class/cs324/)  
- **Climate Change & Renewable Energy**: [NASA Climate Change](https://climate.nasa.gov/)  
- **Animal Behavior & Ecosystems**:  
  - [Smithsonian Open Access](https://www.si.edu/openaccess)  
  - [Wikipedia â€“ Animal Migration](https://en.wikipedia.org/wiki/Animal_migration)  

---

## âš™ï¸ Setup

### 1. Clone the repo
```bash
git clone https://github.com/lkirch/ScienceSage.git
cd ScienceSage
```

### 2. Create a virtual environment
```bash
python3.12 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Configure the environment variables
Copy `.env.example` â†’ `.env` and fill in:
```ini
OPENAI_API_KEY=sk-xxxx
QDRANT_HOST=localhost
QDRANT_PORT=6333
```

### 4. Start Qdrant

You need a running Qdrant vector database for embedding and retrieval.  
You can start Qdrant using Docker:

```bash
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

- This will start Qdrant on `localhost:6333` (REST API) and `localhost:6334` (gRPC).
- Make sure Qdrant is running **before** running any scripts that load collections or query the database.

Alternatively, see [Qdrant documentation](https://qdrant.tech/documentation/quick-start/) for other install options.

### 5. Prepare the data
```bash
python scripts/download_and_clean.py   # fetch & clean NASA/Wikipedia/PDF
python scripts/preprocess.py           # chunk into JSONL
python scripts/embed.py                # embed & store in Qdrant
```

### 6. Run the Streamlit app
```bash
streamlit run app/app.py
```

---

## ğŸ› ï¸ Using the Makefile

This project includes a `Makefile` to simplify common setup and run tasks.

### List available commands
```bash
make help
```

### Typical usage

- **Set up the environment and install dependencies:**
  ```bash
  make install
  ```
- **Prepare the data (download, preprocess, embed):**
  ```bash
  make data
  ```
- **Run the Streamlit app:**
  ```bash
  make run
  ```
- **Clean up generated files:**
  ```bash
  make clean
  ```

> **Note:**  
> You still need to [start Qdrant](#4-start-qdrant) before running any commands that interact with the vector database.

See the `Makefile` for more available targets and details.

---

## ğŸ§ª Running Tests

Unit and integration tests are located in the `tests/` directory and use [pytest](https://docs.pytest.org/).

### Run all tests
```bash
pytest
```

### Run tests with verbose output
```bash
pytest -v
```

### Run a specific test file
```bash
pytest tests/test_pipeline.py
```

> **Tip:**  
> Make sure your virtual environment is activated and all dependencies are installed before running tests.

Some integration tests require a running Qdrant instance and a valid OpenAI API key.  
You can skip these by default, or set the required environment variables to enable