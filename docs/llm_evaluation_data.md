# ü§ñ ScienceSage LLM Evaluation Results

This document describes the structure of LLM (Large Language Model) evaluation results in ScienceSage. These results capture the quality and appropriateness of answers generated by the LLM for various queries and complexity levels.

Evaluation records are stored in `data/eval/llm_eval.jsonl`, with one JSON object per line.

---

## üìÑ Example LLM Evaluation Record

```json
{
  "query": "What was the first animal in space?",
  "level": "middle_school",
  "llm_answer": "The first animal in space was Laika, a dog sent by the Soviet Union in 1957.",
  "ground_truth": "Laika, a Soviet space dog, became the first animal to orbit the Earth in 1957.",
  "retrieved_chunks": [
    "90a4913a-d951-5c8d-ac1b-ca7ff747285b"
  ],
  "feedback": "upvote",
  "metrics": {
    "exact_match": true,
    "f1_score": 0.92,
    "rouge_l": 0.89
  },
  "comments": "Answer is accurate and phrased simply for the target level."
}
```

---

## üè∑Ô∏è Field Descriptions

- **query**: The user or test question.
- **level**: The requested answer complexity (e.g., "middle_school", "college", "advanced").
- **llm_answer**: The answer generated by the LLM.
- **ground_truth**: The reference answer for evaluation.
- **retrieved_chunks**: List of chunk IDs used as context for the answer.
- **feedback**: User or evaluator feedback (e.g., "upvote", "downvote").
- **metrics**: Automated evaluation metrics (e.g., exact match, F1 score, ROUGE-L).
- **comments**: Additional notes or qualitative feedback.

---

## üìÇ Location

All LLM evaluation records are stored in [data/eval/llm_eval.jsonl](../data/eval/llm_eval.jsonl), one JSON object per line.

---