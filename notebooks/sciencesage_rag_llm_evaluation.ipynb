{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2ad271",
   "metadata": {},
   "source": [
    "### RAG LLM Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305540c9",
   "metadata": {},
   "source": [
    "ScienceSage RAG LLM Evaluation Metrics\n",
    "\n",
    "This notebook computes and visualizes key retrieval metrics for your RAG pipeline:\n",
    "- Precision@k\n",
    "- Recall@k\n",
    "- Mean Reciprocal Rank (MRR)\n",
    "- Normalized Discounted Cumulative Gain (nDCG)\n",
    "- Contextual Recall and Sufficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4549c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../sciencesage\")\n",
    "from sciencesage.config import GOLDEN_DATA_FILE, EVAL_RESULTS_FILE, TOP_K, METRICS_SUMMARY_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebbe66",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/eval/golden_dataset.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [json.loads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m line.strip()]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m golden = \u001b[43mload_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOLDEN_DATA_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m results = load_jsonl(EVAL_RESULTS_FILE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_jsonl\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_jsonl\u001b[39m(path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [json.loads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m line.strip()]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ScienceSage/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/eval/golden_dataset.jsonl'"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path) as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "golden = load_jsonl(\"../\" + GOLDEN_DATA_FILE)\n",
    "results = load_jsonl(\"../\" + EVAL_RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb19443",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_df = pd.DataFrame(golden)\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    return len([chunk for chunk in retrieved_k if chunk in relevant_set]) / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9550dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    return len([chunk for chunk in retrieved_k if chunk in relevant_set]) / len(relevant_set) if relevant_set else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a5088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(retrieved, relevant):\n",
    "    for idx, chunk in enumerate(retrieved, 1):\n",
    "        if chunk in relevant:\n",
    "            return 1.0 / idx\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(retrieved, relevant, k):\n",
    "    dcg_val = 0.0\n",
    "    for i, chunk in enumerate(retrieved[:k]):\n",
    "        rel = 1 if chunk in relevant else 0\n",
    "        dcg_val += rel / np.log2(i + 2)\n",
    "    return dcg_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    ideal_rels = [1] * min(len(relevant), k)\n",
    "    ideal_dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal_rels)])\n",
    "    if ideal_dcg == 0:\n",
    "        return 0.0\n",
    "    return dcg(retrieved, relevant, k) / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81234414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual Recall and Sufficiency: For demo, treat as recall@k (customize as needed)\n",
    "def contextual_recall_and_sufficiency(retrieved, relevant, k):\n",
    "    # Placeholder: in practice, this may require human or LLM judgment\n",
    "    return recall_at_k(retrieved, relevant, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54959aab",
   "metadata": {},
   "source": [
    "### Compute Metrics for All Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff184d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for g, r in zip(golden, results):\n",
    "    retrieved = r.get(\"retrieved_chunks\", [])\n",
    "    relevant = g.get(\"ground_truth_chunks\", [])\n",
    "    metrics.append({\n",
    "        \"query\": g.get(\"query\", \"\"),\n",
    "        f\"precision@{TOP_K}\": precision_at_k(retrieved, relevant, TOP_K),\n",
    "        f\"recall@{TOP_K}\": recall_at_k(retrieved, relevant, TOP_K),\n",
    "        \"MRR\": reciprocal_rank(retrieved, relevant),\n",
    "        \"nDCG\": ndcg_at_k(retrieved, relevant, TOP_K),\n",
    "        \"contextual_recall_sufficiency\": contextual_recall_and_sufficiency(retrieved, relevant, TOP_K)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbfa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics = metrics_df.mean(numeric_only=True)\n",
    "print(\"Average Metrics:\")\n",
    "display(agg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of average metrics\n",
    "agg_metrics.plot(kind=\"bar\", figsize=(8,4), ylim=(0,1), title=\"Average RAG Retrieval Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f07b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plots\n",
    "metrics_df[[f\"precision@{TOP_K}\", f\"recall@{TOP_K}\", \"MRR\", \"nDCG\", \"contextual_recall_sufficiency\"]].plot.hist(alpha=0.7, bins=10, figsize=(10,5), title=\"Metric Distributions\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159507c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics by Topic or Level\n",
    "if \"topic\" in golden_df.columns:\n",
    "    merged = pd.concat([metrics_df, golden_df[[\"topic\"]]], axis=1)\n",
    "    topic_means = merged.groupby(\"topic\").mean(numeric_only=True)\n",
    "    topic_means.plot(kind=\"bar\", figsize=(12,6), title=\"Metrics by Topic\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c809279",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(METRICS_SUMMARY_FILE, index=False)\n",
    "print(f\"Saved metrics summary to {METRICS_SUMMARY_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
